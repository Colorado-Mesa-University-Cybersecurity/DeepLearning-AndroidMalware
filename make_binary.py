#! /bin/python3

from fastai.tabular import *
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import os
import sys
import glob
from sklearn.utils import shuffle
import glob
import argparse

def loadData(csvFile):
    pickleDump = '{}.pickle'.format(csvFile)
    if os.path.exists(pickleDump):
        df = pd.read_pickle(pickleDump)
    else:
        print('reached csv:' + csvFile)
        df = pd.read_csv(csvFile, low_memory=False)
        # clean data
        # strip the whitspaces from column names
        df = df.rename(str.strip, axis='columns')
        df.drop(columns=['Flow ID', 'Source IP', 'Destination IP', 'Timestamp'], inplace=True)
        # drop missing values/NaN etc.
        df.dropna(inplace=True)
        # drop Infinity rows and NaN string from each column
        for col in df.columns:
            indexNames = df[df[col]=='Infinity'].index
            if not indexNames.empty:
                print('deleting {} rows with Infinity in column {}'.format(len(indexNames), col))
                df.drop(indexNames, inplace=True)
            indexNames = df[df[col]=='NaN'].index
            if not indexNames.empty:
                print('deleting {} rows with NaN in column {}'.format(len(indexNames), col))
                df.drop(indexNames, inplace=True)
        
        # convert  Flow Bytes/s object & Flow Packets/s object into float type
        df['Flow Bytes/s'] = df['Flow Bytes/s'].astype('float64')
        df['Flow Packets/s'] = df['Flow Packets/s'].astype('float64')
        #print(df.tail())
        df.to_pickle(pickleDump)
    
    return df

def loadAllData(root):
    print('reached with root=' + root)
    pickleDump = '{}.pickle'.format(root)
    #if os.path.exists(pickleDump):
    #    return pd.read_pickle(pickleDump)
    print('reached because pickleDump doesnt exist')
    
    folders = os.listdir(root)
    #print(folders)
    #files = glob.glob(os.path.join(root, 'Dowgin/') + '*.csv')
    #print(files)
    df = pd.DataFrame()
    print('reached dataframe as df:{}'.format(df))
    print('with folders={}'.format(folders))
    for folder in folders:
	# Need to do second level folders
        if os.path.isdir(os.path.join(root, folder)):
            folders2 = os.listdir(root + '/' + folder)
            for folder2 in folders2:
                files = glob.glob(os.path.join(root + '/' + folder, folder2) + "/*.csv")
                if not files:
                    print('continuing on {}'.format(folder2))
                    continue
                if df.empty:
                    df = loadData(files[0])
                for file in files[1:]:
                    df1 = loadData(file)
                    df = df.append(df1, ignore_index=True)
    df.to_pickle(pickleDump)
    return df

def make_network(layer_count):
	network = []
	for power in range(layer_count+1):
		network.append(pow(2, power+5))
	network.reverse()
	return network

def train_on(data_set='', layers=0):
	df = loadAllData('dataset/' + data_set)
	dataPath = 'dataset/'+ data_set
	dep_var = 'Label'
	cat_names = ['Source Port', 'Destination Port', 'Protocol']
	cont_names = list(set(df.columns) - set(cat_names) - set([dep_var]))

	network = make_network(layers)

	procs = [FillMissing, Categorify, Normalize]
	sss = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=0)
	print(sss)

	print('Training on {} with network {}'.format(data_set, network))

	#list of columns giving us issue. I'm removing them for the time being
	malcontents = ['Fwd Avg Bytes/Bulk', 'Flow IAT Min', 'CWE Flag Count', 'Down/Up Ratio', 'Packet Length Std']
	#df.drop(columns=malcontents, axis=1, inplace=True)
	#for mal in malcontents:
	#	cont_names.remove(mal)

	#if data_set == 'binary':
	print(df)
	#for label in df['Label']:
	#	print(label)

	'''for train_idx, test_idx in sss.split(df.index, df[dep_var]):
	    data_fold = (TabularList.from_df(df, path=dataPath, cat_names=cat_names, cont_names=cont_names, procs=procs)
			     .split_by_idxs(train_idx, test_idx)
			     .label_from_df(cols=dep_var)
			     .databunch())'''
	print('reached')
	'''    # create model and learn
	    model = tabular_learner(data_fold, layers=network, metrics=accuracy, callback_fns=ShowGraph)
	    model.fit_one_cycle(cyc_len=10) 
            #model.save('{}_{}.model'.format(data_set, os.path.basename(dataPath)))
	'''

if __name__ == '__main__':
	parser = argparse.ArgumentParser()
	parser.add_argument('--dataset', help='data folder to train on')
	args = parser.parse_args()

	if args.dataset:
		train_on(args.dataset)
	else:
		parser.print_help()
