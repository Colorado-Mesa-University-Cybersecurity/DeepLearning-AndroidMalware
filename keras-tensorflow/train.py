#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np

from keras.models import Sequential, load_model
from keras.layers import Dense, Activation
from sklearn.model_selection import KFold, train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical, normalize
from sklearn.utils import shuffle

print('Imports complete')


# In[2]:


df = pd.read_csv('../../malware_dataset/complete_binarized_dataset_cleaned.csv', index_col=0)
df.head()


# In[3]:


df['Label_binarized'].value_counts()


# In[4]:


dep_var = 'Label_binarized'
model_name = 'init'
cont_names = list( set(df.columns) - set([dep_var]) )
print(cont_names)


# In[5]:


sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)
print(sss)

df_y = df[dep_var]
del df[dep_var]

encoder = LabelEncoder()
encoder.fit(df_y)
df_y = encoder.transform(df_y)
data_x = (df - df.mean()) / (df.max() - df.min())
data_x = data_x.values


# In[6]:


model = Sequential()
model.add(Dense(32, activation='relu', input_shape=(9,)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='softmax'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


# In[7]:


#for train_idx, test_idx in sss.split(X=np.zeros(data_x.shape[0]), y=df_y.shape[0]):
#    X_train, X_test = data_x[train_idx], data_x[test_idx]
#    y_train, y_test = df_y[train_idx], df_y[test_idx]
#    
#    model.fit(x=X_train, y=y_train, epochs=10, batch_size=10, verbose=2, validation_data=(X_test, y_test))

for train_idx, test_idx in KFold(10).split(df):
    X_train, X_test = data_x[train_idx], data_x[test_idx]
    y_train, y_test = df_y[train_idx], df_y[test_idx]

    model.fit(x=X_train, y=y_train, epochs=10, batch_size=10, verbose=1, validation_data=(X_test, y_test))

    
scores = model.evaluate(X_test, y_test, verbose=1)
print(model.metrics_names)
acc, loss = scores[1]*100, scores[0]*100
print('Baseline: accuracy: {:.2f}%: loss: {:2f}'.format(acc, loss))

