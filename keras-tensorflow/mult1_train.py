import pandas as pd
import numpy as np
import keras

from keras.models import Sequential, load_model
from keras.layers import Dense, Activation, Dropout, Conv1D, MaxPooling1D, Flatten
from keras.metrics import CategoricalAccuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from keras.utils.np_utils import to_categorical, normalize
from sklearn.utils import shuffle

print('Imports complete')

# Pulls in the dataset from a previously-saved csv
df = pd.read_csv('../../malware_dataset/BMA.csv', index_col=0)
df.head()

df.tail()

df['Label_downsampled'].value_counts()

dep_var = 'Label_downsampled'
model_name = 'init'
cont_names = list( set(df.columns) - set([dep_var]) )
#print(cont_names)

le = LabelEncoder()
le.fit(df[dep_var])
le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(le_name_mapping)

# Set df_y (datafold-y values) to the target classification 'dep_var' and remove this column from df
#   x_data ~= df
#   y_data ~= df_y
df_y = df[dep_var]
del df[dep_var]

"""# Encodes target classifications from 0 to n-1 integers (only use for target classifications)
#encoder = LabelEncoder()
encoder = LabelEncoder()
encoder.fit(df_y)

# Transforms the y data to be encoded
data_y = encoder.transform(df_y)
data_y = to_categorical(data_y)"""

"""# encode class values as integers
encoder = LabelEncoder()
data_y = encoder.fit_transform(df_y)
encoded_Y = encoder.transform(df_y)
# convert integers to dummy variables (i.e. one hot encoded)
data_y = to_categorical(encoded_Y)"""

label_encoder = LabelEncoder()
data_y = label_encoder.fit_transform(df_y)

# Normalize the x data
data_x = (df - df.mean()) / (df.max() - df.min())
data_x = data_x.values

# Only run this cell once!
#print(data_x.shape)
shape = data_x.shape
data_x = data_x.reshape(shape[0], shape[1], 1)
print(data_x.shape)

# Only run this cell once!
fold_num = 1
total_folds = 10


# Stratified K-fold here because 10-fold cv is generally recommended and Stratified will maintain the 
#  target classification categorical balances for each fold
for train_idx, test_idx in StratifiedKFold(n_splits=total_folds, shuffle=True, random_state=1).split(data_x, data_y):
    if fold_num == 1:
        data_y = to_categorical(data_y)
    
    print('Fold {}/{}'.format(fold_num, total_folds))
    fold_num += 1

    # Set up the training and testing sets
    X_train, X_test = data_x[train_idx], data_x[test_idx]
    y_train, y_test = data_y[train_idx], data_y[test_idx]

    # Set up the metrics we want to collect
    accuracy = CategoricalAccuracy() 	# Will change this to Categorical if the target classification is categorical
    tp = TruePositives()		# These could be collected with a confusion matrix, however translating back
    tn = TrueNegatives()		#  and forth from an image may be frustrating (it was last time I did it)
    fp = FalsePositives()
    fn = FalseNegatives()
    metrics = [accuracy, tp, tn, fp, fn]

    # The model must be reinitialized otherwise the model will have trained on all of the data (that wouldn't be true 10-fold cv)
    model = Sequential()
    #model.add(Dense(128, input_shape=(9,))) 	# Input layer, needs same shape as input data (9 values 1D)
    model.add(Conv1D(64, kernel_size=4, input_shape=(9, 1)))
    model.add(MaxPooling1D())
    model.add(Conv1D(32, kernel_size=2))
    model.add(MaxPooling1D())
    model.add(Flatten())
    model.add(Dense(64, activation='sigmoid'))			# Hidden layer of nodes
    model.add(Dense(32, activation='selu'))			# Hidden layer of nodes
    model.add(Dense(3, activation='softmax'))			# Output layer of only one node (on/off)

    # "Configures the model for training"
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)
    
    # Stochastic gradient descent and momentum optimizer.
    #sgd = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)

    # mean squared error computes the mean of absolute difference between labels and predictions
    #model.compile(optimizer=sgd, loss='mean_squared_error', metrics=metrics)

    # Fit and test the model 
    model.fit(x=X_train, y=y_train, epochs=100, batch_size=512, verbose=1, validation_data=(X_test, y_test))
    
    # Evaluate the performance of the model on the test set
    scores = model.evaluate(X_test, y_test, verbose=2)
    #print(model.metrics_names)
    #print('Scores values: {}'.format(scores))
    acc, loss, tpn, tnn, fpn, fnn = scores[1]*100, scores[0]*100, scores[2], scores[3], scores[4], scores[5]
    acc, loss = scores[1]*100, scores[0]*100
    totaln = tpn + tnn + fpn + fnn
    print('Baseline: accuracy: {:.2f}%: loss: {:2f}'.format(acc, loss))
    print('\tTrue Positive Rate: {} ({})'.format(tpn/totaln, tpn))
    print('\tTrue Negative Rate: {} ({})'.format(tnn/totaln, tnn))
    print('\tFalse Positive Rate: {} ({})'.format(fpn/totaln, fpn))
    print('\tFalse Negative Rate: {} ({})'.format(fnn/totaln, fnn))


