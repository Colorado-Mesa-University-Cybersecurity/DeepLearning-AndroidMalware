import pandas as pd
import numpy as np
import keras

from keras.models import Sequential, load_model
from keras.layers import Dense, Activation, Dropout
from keras.metrics import BinaryAccuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical, normalize
from sklearn.utils import shuffle

print('Imports complete')

# Pulls in the dataset from a previously-saved csv
df = pd.read_csv('../malware_dataset/complete_binarized_dataset_cleaned.csv', index_col=0)
df.head()

#df['Label_binarized'].value_counts()

dep_var = 'Label_binarized'
model_name = 'init'
cont_names = list( set(df.columns) - set([dep_var]) )
#print(cont_names)

# Set df_y (datafold-y values) to the target classification 'dep_var' and remove this column from df
#   x_data ~= df
#   y_data ~= df_y
df_y = df[dep_var]
del df[dep_var]

# Encodes target classifications from 0 to n-1 integers (only use for target classifications)
encoder = LabelEncoder()
encoder.fit(df_y)

# Transforms the y data to be encoded
data_y = encoder.transform(df_y)

# Normalize the x data
data_x = (df - df.mean()) / (df.max() - df.min())
data_x = data_x.values

fold_num = 1
total_folds = 10

# Stratified K-fold here because 10-fold cv is generally recommended and Stratified will maintain the 
#  target classification categorical balances for each fold
for train_idx, test_idx in StratifiedKFold(n_splits=total_folds, shuffle=True, random_state=1).split(data_x, data_y):
    print('Fold {}/{}'.format(fold_num, total_folds))
    fold_num += 1

    # Set up the training and testing sets
    X_train, X_test = data_x[train_idx], data_x[test_idx]
    y_train, y_test = data_y[train_idx], data_y[test_idx]

    # Set up the metrics we want to collect
    accuracy = BinaryAccuracy() 	# Will change this to Categorical if the target classification is categorical
    tp = TruePositives()		# These could be collected with a confusion matrix, however translating back
    tn = TrueNegatives()		#  and forth from an image may be frustrating (it was last time I did it)
    fp = FalsePositives()
    fn = FalseNegatives()
    metrics = [accuracy, tp, tn, fp, fn]

    # The model must be reinitialized otherwise the model will have trained on all of the data (that wouldn't be true 10-fold cv)
    model = Sequential()
    model.add(Dense(128, input_shape=(9,))) 	# Input layer, needs same shape as input data (9 values 1D)
    model.add(Dense(64, activation='relu'))			# Hidden layer of nodes
    model.add(Dense(32, activation='relu'))			# Hidden layer of nodes
    model.add(Dense(1, activation='sigmoid'))			# Output layer of only one node (on/off)

    # "Configures the model for training"
    # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)
    
    # Stochastic gradient descent and momentum optimizer.
    sgd = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)

    # mean squared error computes the mean of absolute difference between labels and predictions
    model.compile(optimizer=sgd, loss='mean_squared_error', metrics=['accuracy'])

    # Fit and test the model 
    model.fit(x=X_train, y=y_train, epochs=1, batch_size=512, verbose=0, validation_data=(X_test, y_test))
    
    # Evaluate the performance of the model on the test set
    scores = model.evaluate(X_test, y_test, verbose=2)
    #print(model.metrics_names)
    #print('Scores values: {}'.format(scores))
    # acc, loss, tpn, tnn, fpn, fnn = scores[1]*100, scores[0]*100, scores[2], scores[3], scores[4], scores[5]
    acc, loss = scores[1]*100, scores[0]*100
    # totaln = tpn + tnn + fpn + fnn
    print('Baseline: accuracy: {:.2f}%: loss: {:2f}'.format(acc, loss))
    # print('\tTrue Positive Rate: {} ({})'.format(tpn/totaln, tpn))
    # print('\tTrue Negative Rate: {} ({})'.format(tnn/totaln, tnn))
    # print('\tFalse Positive Rate: {} ({})'.format(fpn/totaln, fpn))
    # print('\tFalse Negative Rate: {} ({})'.format(fnn/totaln, fnn))

