#! /bin/python3

from fastai.tabular import *
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import os
import sys
import glob
from sklearn.utils import shuffle
import glob
import argparse

def loadData(csvFile):
    pickleDump = '{}.pickle'.format(csvFile)
    if os.path.exists(pickleDump):
        df = pd.read_pickle(pickleDump)
    else:
        df = pd.read_csv(csvFile, low_memory=False)
        # clean data
        # strip the whitspaces from column names
        df = df.rename(str.strip, axis='columns')
        df.drop(columns=['Flow ID', 'Source IP', 'Destination IP', 'Timestamp'], inplace=True)
        # drop missing values/NaN etc.
        df.dropna(inplace=True)
        # drop Infinity rows and NaN string from each column
        for col in df.columns:
            indexNames = df[df[col]=='Infinity'].index
            if not indexNames.empty:
                print('deleting {} rows with Infinity in column {}'.format(len(indexNames), col))
                df.drop(indexNames, inplace=True)
            indexNames = df[df[col]=='NaN'].index
            if not indexNames.empty:
                print('deleting {} rows with NaN in column {}'.format(len(indexNames), col))
                df.drop(indexNames, inplace=True)
        
        # convert  Flow Bytes/s object & Flow Packets/s object into float type
        df['Flow Bytes/s'] = df['Flow Bytes/s'].astype('float64')
        df['Flow Packets/s'] = df['Flow Packets/s'].astype('float64')
        #print(df.tail())
        df.to_pickle(pickleDump)
    
    return df

def loadAllData(root):
    pickleDump = '{}.pickle'.format(root)
    if os.path.exists(pickleDump):
        return pd.read_pickle(pickleDump)
    
    folders = os.listdir(root)
    #print(folders)
    #files = glob.glob(os.path.join(root, 'Dowgin/') + '*.csv')
    #print(files)
    df = pd.DataFrame()
    for folder in folders:
        files = glob.glob(os.path.join(root, folder) + "/*.csv")
        if not files:
            continue
        if df.empty:
            df = loadData(files[0])
        for file in files[1:]:
            df1 = loadData(file)
            df = df.append(df1, ignore_index=True)
    df.to_pickle(pickleDump)
    return df

def train_on(data_set=''):
	df = loadAllData('dataset/' + data_set)
	dataPath = 'dataset/' + data_set
	dep_var = 'Label'
	cat_names = ['Source Port', 'Destination Port', 'Protocol']
	cont_names = list(set(df.columns) - set(cat_names) - set([dep_var]))

	procs = [FillMissing, Categorify, Normalize]
	sss = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=0)
	print(sss)

	for train_idx, test_idx in sss.split(df.index, df[dep_var]):
	    data_fold = (TabularList.from_df(df, path=dataPath, cat_names=cat_names, cont_names=cont_names, procs=procs)
			     .split_by_idxs(train_idx, test_idx)
			     .label_from_df(cols=dep_var)
			     .databunch())
	    # create model and learn
	    model = tabular_learner(data_fold, layers=[200, 200], metrics=accuracy, callback_fns=ShowGraph)
	    model.fit_one_cycle(cyc_len=10) 
            #model.save('{}_{}.model'.format(data_set, os.path.basename(dataPath)))

if __name__ == '__main__':
	parser = argparse.ArgumentParser()
	parser.add_argument('--dataset', help='data folder to train on')
	args = parser.parse_args()

	if args.dataset:
		print('Training on {}'.format(args.dataset))
		train_on(args.dataset)	
