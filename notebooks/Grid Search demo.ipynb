{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.metrics import CategoricalAccuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unnecessary columns for chunk 1...done\n",
      "Dropping unnecessary columns for chunk 2...done\n",
      "Dropping unnecessary columns for chunk 3...done\n",
      "Dropping unnecessary columns for chunk 4...done\n",
      "Dropping unnecessary columns for chunk 5...done\n",
      "Dropping unnecessary columns for chunk 6...done\n",
      "Dropping unnecessary columns for chunk 7...done\n",
      "Dropping unnecessary columns for chunk 8...done\n",
      "Dropping unnecessary columns for chunk 9...done\n",
      "Dropping unnecessary columns for chunk 10...done\n",
      "\n",
      "Dataset Composition:\n",
      "BENIGN    229275\n",
      "ADWARE    229275\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Active Features:\n",
      "\t1. Total Length of Fwd Packets\n",
      "\t2. Total Length of Bwd Packets\n",
      "\t3. Bwd Packet Length Mean\n",
      "\t4. Max Packet Length\n",
      "\t5. Packet Length Mean\n",
      "\t6. Packet Length Std\n",
      "\t7. Packet Length Variance\n",
      "\t8. Avg Bwd Segment Size\n",
      "\t9. Subflow Fwd Bytes\n",
      "\t10. Subflow Bwd Bytes\n",
      "\t11. Init_Win_bytes_backward\n"
     ]
    }
   ],
   "source": [
    "# Import the data file\n",
    "path = '../../malware_dataset/'\n",
    "datafile = 'adware_vs_benign.csv'\n",
    "\n",
    "# Technique acquired from https://towardsdatascience.com/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c\n",
    "# I'm using this chunk technique because these files are kind of large and I want to make it as easy on us as\n",
    "#  possible.\n",
    "df_chunk = pd.read_csv(path + datafile, chunksize=50000)\n",
    "chunk_list = []  # append each chunk df here \n",
    "\n",
    "cols_to_keep = ['Init_Win_bytes_backward',\n",
    "               'Packet Length Mean',\n",
    "               'Max Packet Length',\n",
    "               'Subflow Fwd Bytes',\n",
    "               'Total Length of Fwd Packets',\n",
    "               'Subflow Bwd Bytes',\n",
    "               'Total Length of Bwd Packets',\n",
    "               'Avg Bwd Segment Size',\n",
    "               'Bwd Packet Length Mean',\n",
    "               'Packet Length Variance',\n",
    "               'Packet Length Std']\n",
    "cols_to_keep.append('Label')\n",
    "\n",
    "def filter_columns(chk, chknum):\n",
    "    # Clean the data for the features we want  \n",
    "    print('Dropping unnecessary columns for chunk {}...'.format(chknum), end='')\n",
    "    chk.drop(columns=[col for col in chk.columns if col not in cols_to_keep], inplace=True, errors='raise') # Some systems raise an error about this errors='raise' argument (which is ironic, I guess). Feel free to remove it if need be.\n",
    "    chk.dropna(inplace=True)\n",
    "    print('done')\n",
    "    \n",
    "    return chk\n",
    "\n",
    "# Each chunk is in df format\n",
    "chunkn = 1\n",
    "for chunk in df_chunk:  \n",
    "    # perform data filtering \n",
    "    chunk_filter = filter_columns(chunk, chunkn)\n",
    "    chunkn += 1\n",
    "    \n",
    "    # Once the data filtering is done, append the chunk to list\n",
    "    chunk_list.append(chunk_filter)\n",
    "    \n",
    "# concat the list into dataframe \n",
    "df = pd.concat(chunk_list)\n",
    "\n",
    "dep_var = 'Label'\n",
    "print('\\nDataset Composition:\\n{}\\n'.format(df[dep_var].value_counts()))\n",
    "\n",
    "# Output the features we have active\n",
    "print('Active Features:')\n",
    "n=1\n",
    "for col in df.columns[:-1]:\n",
    "    print('\\t{}. {}'.format(n, col))\n",
    "    n+=1\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "[[ 9.66793904e-03  3.83955293e-02  3.83955293e-02  3.83955293e-02\n",
      "   1.92438025e-02  1.65858820e-02  9.95889797e-01  3.83955293e-02\n",
      "   9.66793904e-03  3.83955293e-02 -2.76226830e-04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -1.00000000e+00]\n",
      " [ 0.00000000e+00  2.93162346e-02  1.46581173e-02  2.93162346e-02\n",
      "   7.32905865e-03  1.46581173e-02  2.27200818e-01  1.46581173e-02\n",
      "   0.00000000e+00  2.93162346e-02  9.72164167e-01]\n",
      " [ 6.23200326e-03  3.07579516e-02  3.07579516e-02  3.07579516e-02\n",
      "   1.44073194e-02  1.41600629e-02  9.97388564e-01  3.07579516e-02\n",
      "   6.23200326e-03  3.07579516e-02 -2.01032363e-04]]\n",
      "Targets:\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Separate the data out into the data and target classification\n",
    "X = normalize( ( df.loc[:, df.columns != dep_var] ).values )\n",
    "y = df[dep_var]\n",
    "# One-Hot Encoding for the target classification\n",
    "label_encoder = LabelEncoder()\n",
    "#y = label_encoder.fit_transform(y)\n",
    "y = y.map(lambda classif : 0 if classif == 'BENIGN' else 1)\n",
    "y = keras.utils.to_categorical(y, num_classes=y.nunique())\n",
    "\n",
    "print('Data:')\n",
    "print(X[:5, :])\n",
    "print('Targets:')\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset up into training and testing sets\n",
    "#  This split is not stratified by default according to \n",
    "#  the documentation found here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, stratify=y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network model training and evaluation\n",
    "def build_fit_eval(opt, act):\n",
    "    # Set up the metrics we want to collect\n",
    "    accuracy = CategoricalAccuracy() \t# Will change this to Categorical if the target classification is categorical\n",
    "    tp = TruePositives()\t\t# These could be collected with a confusion matrix, however translating back\n",
    "    tn = TrueNegatives()\t\t#  and forth from an image may be frustrating (it was last time I did it)\n",
    "    fp = FalsePositives()\n",
    "    fn = FalseNegatives()\n",
    "    metrics = [accuracy, tp, tn, fp, fn]\n",
    "\n",
    "    # The model must be reinitialized otherwise the model will have trained on all of the data (that wouldn't be true 10-fold cv)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(len(cols_to_keep) - 1 ,))) \t# Input layer, needs same shape as input data (9 values 1D)\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(Dense(2, activation=act))\t\t\t# Output layer of 3 nodes \n",
    "\n",
    "    # \"Configures the model for training\"\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=metrics)\n",
    "\n",
    "    # Fit and test the model \n",
    "    model.fit(x=X_train, y=y_train, epochs=20, batch_size=512, verbose=0, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate the performance of the model on the test set\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc, loss, tpn, tnn, fpn, fnn = scores[1]*100, scores[0]*100, scores[2], scores[3], scores[4], scores[5]\n",
    "    totaln = tpn + tnn + fpn + fnn\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers=[\n",
    "    'SGD',\n",
    "    'RMSprop',\n",
    "    'Adam',\n",
    "    'Adadelta',\n",
    "    'Adagrad',\n",
    "    'Adamax',\n",
    "    'Nadam'\n",
    "]\n",
    "activations=[\n",
    "    'relu',\n",
    "    'sigmoid',\n",
    "    'softmax',\n",
    "    'softplus',\n",
    "    'softsign',\n",
    "    'tanh',\n",
    "    'selu',\n",
    "    'elu',\n",
    "    'exponential'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer\tActivation\tAccuracy (%)\n",
      "\n",
      "SGD\t\trelu\t\t50.00\n",
      "\t\tsigmoid\t\t52.92\n",
      "\t\tsoftmax\t\t52.82\n",
      "\t\tsoftplus\t\t52.87\n",
      "\t\tsoftsign\t\t50.00\n",
      "\t\ttanh\t\t50.00\n",
      "\t\tselu\t\t50.00\n",
      "\t\telu\t\t50.00\n",
      "\t\texponential\t\t52.95\n",
      "RMSprop\t\trelu\t\t50.00\n",
      "\t\tsigmoid\t\t53.61\n",
      "\t\tsoftmax\t\t53.66\n",
      "\t\tsoftplus\t\t53.95\n",
      "\t\tsoftsign\t\t50.00\n",
      "\t\ttanh\t\t50.00\n",
      "\t\tselu\t\t50.00\n",
      "\t\telu\t\t50.00\n",
      "\t\texponential\t\t53.51\n",
      "Adam\t\trelu\t\t50.00\n",
      "\t\tsigmoid\t\t53.35\n",
      "\t\tsoftmax\t\t53.75\n",
      "\t\tsoftplus\t\t53.61\n",
      "\t\tsoftsign\t\t50.00\n",
      "\t\ttanh\t\t50.00\n",
      "\t\tselu\t\t50.00\n",
      "\t\telu\t\t50.00\n",
      "\t\texponential\t\t54.22\n",
      "Adadelta\t\trelu\t\t50.00\n",
      "\t\tsigmoid\t\t52.26\n",
      "\t\tsoftmax\t\t53.70\n",
      "\t\tsoftplus\t\t53.71\n",
      "\t\tsoftsign\t\t50.00\n",
      "\t\ttanh\t\t50.00\n",
      "\t\tselu\t\t50.00\n",
      "\t\telu\t\t50.00\n",
      "\t\texponential\t\t53.83\n",
      "Adagrad\t\trelu\t\t50.00\n",
      "\t\tsigmoid\t\t53.13\n",
      "\t\tsoftmax\t\t53.32\n",
      "\t\tsoftplus\t\t53.43\n",
      "\t\tsoftsign\t\t50.00\n",
      "\t\ttanh\t\t50.00\n",
      "\t\tselu\t\t50.00\n",
      "\t\telu\t\t50.00\n",
      "\t\texponential\t\t53.10\n",
      "Adamax\t\trelu\t\t50.00\n",
      "\t\tsigmoid\t\t53.25\n",
      "\t\tsoftmax\t\t53.55\n",
      "\t\tsoftplus\t\t53.61\n",
      "\t\tsoftsign\t\t50.00\n",
      "\t\ttanh\t\t50.00\n",
      "\t\tselu\t\t50.00\n",
      "\t\telu\t\t50.00\n",
      "\t\texponential\t\t53.79\n",
      "Nadam\t\trelu\t\t50.00\n",
      "\t\tsigmoid\t\t53.99\n",
      "\t\tsoftmax\t\t53.97\n",
      "\t\tsoftplus\t\t53.82\n",
      "\t\tsoftsign\t\t50.00\n",
      "\t\ttanh\t\t50.00\n",
      "\t\tselu\t\t50.00\n",
      "\t\telu\t\t50.00\n",
      "\t\texponential\t\t53.79\n"
     ]
    }
   ],
   "source": [
    "print('Optimizer\\tActivation\\tAccuracy (%)\\n')\n",
    "for opt in optimizers:\n",
    "    print(opt, end='')\n",
    "    for act in activations:\n",
    "        print('\\t\\t{}'.format(act), end='')\n",
    "        acc = build_fit_eval(opt, act)\n",
    "        print('\\t\\t{:.2f}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
