{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import BinaryAccuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "\n",
    "\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADWARE    229275\n",
      "BENIGN    229275\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import the data file\n",
    "path = '../../malware_dataset/'\n",
    "df = pd.read_csv(path + 'adware_vs_benign.csv')\n",
    "\n",
    "dep_var = 'Label'\n",
    "print(df[dep_var].value_counts())\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unnecessary columns...done\n",
      "Active Features:\n",
      "\tBwd Packet Length Std\n",
      "\tFlow IAT Max\n",
      "\tFlow IAT Min\n",
      "\tFwd IAT Total\n",
      "\tBwd IAT Total\n",
      "\tFIN Flag Count\n",
      "\tInit_Win_bytes_forward\n",
      "\tmin_seg_size_forward\n",
      "\tIdle Max\n"
     ]
    }
   ],
   "source": [
    "# Clean the data for the features we want\n",
    "cols_to_keep = ['Flow IAT Max',\n",
    "                     'Flow IAT Min',\n",
    "                     'Bwd Packet Length Std',\n",
    "                     'FIN Flag Count',\n",
    "                     'Fwd IAT Total',\n",
    "                     'Bwd IAT Total',\n",
    "                     'Idle Max',\n",
    "                     'Init_Win_bytes_forward',\n",
    "                     'min_seg_size_forward',\n",
    "                     'Label']\n",
    " \n",
    "print('Dropping unnecessary columns...', end='')\n",
    "df.drop(columns=[col for col in df.columns if col not in cols_to_keep], inplace=True, errors='raise') # Some systems raise an error about this errors='raise' argument (which is ironic, I guess). Feel free to remove it if need be.\n",
    "df.dropna(inplace=True)\n",
    "print('done')\n",
    "\n",
    "# Output the features we have active\n",
    "print('Active Features:')\n",
    "for col in df.columns[:-1]:\n",
    "    print('\\t' + col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "[[0.00000000e+00 4.51126810e+07 3.42300000e+03 4.68139810e+07\n",
      "  1.31575800e+06 0.00000000e+00 6.55350000e+04 2.00000000e+01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 4.60000000e+01 4.60000000e+01 4.60000000e+01\n",
      "  0.00000000e+00 0.00000000e+00 1.35000000e+02 3.20000000e+01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 3.58750000e+04 3.58750000e+04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.52000000e+02 3.20000000e+01\n",
      "  0.00000000e+00]\n",
      " [3.84349516e+02 4.25535350e+07 7.00000000e+00 4.29371740e+07\n",
      "  3.12480000e+05 0.00000000e+00 6.55350000e+04 3.20000000e+01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.77517100e+06 3.64600000e+04 3.64600000e+04\n",
      "  0.00000000e+00 0.00000000e+00 2.45800000e+03 2.00000000e+01\n",
      "  0.00000000e+00]]\n",
      "Targets:\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Separate the data out into the data and target classification\n",
    "X = ( df.loc[:, df.columns != dep_var] ).values\n",
    "y = df[dep_var]\n",
    "# One-Hot Encoding for the target classification\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "print('Data:')\n",
    "print(X[:5, :])\n",
    "print('Targets:')\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset up into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model training and evaluation\n",
    "#rf = RandomForestClassifier(max_depth=2, n_estimators=100, random_state=random_state)\n",
    "#rf.fit(X_train, y_train)\n",
    "#print('Random Forest testing accuracy: {:.2f}%'.format(100*rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree model training and evaluation\n",
    "#dt = DecisionTreeClassifier(max_leaf_nodes=25, criterion='entropy', max_features='log2', splitter='random', random_state=random_state)\n",
    "#dt.fit(X_train, y_train)\n",
    "#print('Decision Tree testing accuracy: {:.2f}%'.format(100*dt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost-Random Forest model training and evaluation\n",
    "#adarf = AdaBoostClassifier(base_estimator=rf, n_estimators=100, random_state=random_state)\n",
    "#adarf.fit(X_train, y_train)\n",
    "#print('AdaBoost-Random Forest testing accuracy: {:.2f}%'.format(100*adarf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost-Decision Tree model training and evaluation\n",
    "#adadt = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=random_state)\n",
    "#adadt.fit(X_train, y_train)\n",
    "#print('AdaBoost-Decision Tree testing accuracy: {:.2f}%'.format(100*adadt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: accuracy: 50.88%: loss: 69.174581\n",
      "\tTrue Positive Rate: 0.26899591599370876 (40705.0)\n",
      "\tTrue Negative Rate: 0.23984615587951522 (36294.0)\n",
      "\tFalse Positive Rate: 0.2601340188472264 (39364.0)\n",
      "\tFalse Negative Rate: 0.23102390927954958 (34959.0)\n"
     ]
    }
   ],
   "source": [
    "# Deep Neural Network model training and evaluation\n",
    "\n",
    "# Set up the metrics we want to collect\n",
    "accuracy = BinaryAccuracy() \t# Will change this to Categorical if the target classification is categorical\n",
    "tp = TruePositives()\t\t# These could be collected with a confusion matrix, however translating back\n",
    "tn = TrueNegatives()\t\t#  and forth from an image may be frustrating (it was last time I did it)\n",
    "fp = FalsePositives()\n",
    "fn = FalseNegatives()\n",
    "metrics = [accuracy, tp, tn, fp, fn]\n",
    "\n",
    "# The model must be reinitialized otherwise the model will have trained on all of the data (that wouldn't be true 10-fold cv)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(len(cols_to_keep) - 1 ,))) \t# Input layer, needs same shape as input data (9 values 1D)\n",
    "model.add(Dense(128, activation='relu'))\t\t\t# Hidden layer of nodes\n",
    "model.add(Dense(64, activation='sigmoid'))\t\t\t# Hidden layer of nodes\n",
    "model.add(Dense(32, activation='sigmoid'))\t\t\t# Hidden layer of nodes\n",
    "model.add(Dense(8, activation='sigmoid'))\t\t\t# Hidden layer of nodes\n",
    "model.add(Dense(1, activation='sigmoid'))\t\t\t# Output layer of 3 nodes \n",
    "\n",
    "# \"Configures the model for training\"\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "# Fit and test the model \n",
    "model.fit(x=X_train, y=y_train, epochs=10, batch_size=512, verbose=0, validation_data=(X_test, y_test))\n",
    "    \n",
    "# Evaluate the performance of the model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "acc, loss, tpn, tnn, fpn, fnn = scores[1]*100, scores[0]*100, scores[2], scores[3], scores[4], scores[5]\n",
    "totaln = tpn + tnn + fpn + fnn\n",
    "print('Baseline: accuracy: {:.2f}%: loss: {:2f}'.format(acc, loss))\n",
    "print('\\tTrue Positive Rate: {} ({})'.format(tpn/totaln, tpn))\n",
    "print('\\tTrue Negative Rate: {} ({})'.format(tnn/totaln, tnn))\n",
    "print('\\tFalse Positive Rate: {} ({})'.format(fpn/totaln, fpn))\n",
    "print('\\tFalse Negative Rate: {} ({})'.format(fnn/totaln, fnn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
