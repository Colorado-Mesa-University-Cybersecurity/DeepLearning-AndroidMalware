{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adware vs Benign\n",
    "Here, we are attempting to classify between 'Adware' and 'Benign' using the features generated from the `feature selection` notebook on this data file. While this notebook is specifically tailored for the 'Adware' vs 'Benign' problem, only two items need to be changed for this to be applicable to any other 'vs' file we have available. These will be noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.metrics import CategoricalAccuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "\n",
    "\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data File\n",
    "The `adware_vs_benign.csv` file was generated by the `create benign vs x files` notebook. This notebook will generate all of the data files for these experiments, however modifications must be made upstream to allow for these data files to have all of the available features. Here, we import the data file one chunk at a time since we can filter off the necessary columns every chunk.  \n",
    "\n",
    "If you want to change this notebook for another type of 'vs' file, you would only change a handful of variables:\n",
    "- Change `path` and `datafile` to whatever you need it to be\n",
    "- Either run the `feature_selection` notebook on your given datafile and pull out the list of features <i>or</i> you can add your own below. \n",
    "- That's it, actually. A list without three items is a weak list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unnecessary columns for chunk 1...done\n",
      "Dropping unnecessary columns for chunk 2...done\n",
      "Dropping unnecessary columns for chunk 3...done\n",
      "Dropping unnecessary columns for chunk 4...done\n",
      "Dropping unnecessary columns for chunk 5...done\n",
      "Dropping unnecessary columns for chunk 6...done\n",
      "Dropping unnecessary columns for chunk 7...done\n",
      "Dropping unnecessary columns for chunk 8...done\n",
      "Dropping unnecessary columns for chunk 9...done\n",
      "Dropping unnecessary columns for chunk 10...done\n",
      "\n",
      "Dataset Composition:\n",
      "ADWARE    229275\n",
      "BENIGN    229275\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Active Features:\n",
      "\t1. Source Port\n",
      "\t2. Destination Port\n",
      "\t3. Protocol\n",
      "\t4. Total Fwd Packets\n",
      "\t5. Bwd Packet Length Max\n",
      "\t6. Bwd Packet Length Mean\n",
      "\t7. Bwd Packet Length Std\n",
      "\t8. Fwd PSH Flags\n",
      "\t9. Min Packet Length\n",
      "\t10. Packet Length Mean\n",
      "\t11. Packet Length Std\n",
      "\t12. Packet Length Variance\n",
      "\t13. SYN Flag Count\n",
      "\t14. ACK Flag Count\n",
      "\t15. URG Flag Count\n",
      "\t16. Down/Up Ratio\n",
      "\t17. Average Packet Size\n",
      "\t18. Subflow Fwd Packets\n",
      "\t19. Init_Win_bytes_forward\n",
      "\t20. Init_Win_bytes_backward\n"
     ]
    }
   ],
   "source": [
    "# Import the data file\n",
    "path = '../../malware_dataset/'\n",
    "datafile = 'adware_vs_benign.csv'\n",
    "\n",
    "# Technique acquired from https://towardsdatascience.com/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c\n",
    "# I'm using this chunk technique because these files are kind of large and I want to make it as easy on us as\n",
    "#  possible.\n",
    "df_chunk = pd.read_csv(path + datafile, chunksize=50000)\n",
    "chunk_list = []  # append each chunk df here \n",
    "\n",
    "cols_to_keep = ['Down/Up Ratio', \n",
    "                'Fwd PSH Flags', \n",
    "                'SYN Flag Count', \n",
    "                'ACK Flag Count', \n",
    "                'Packet Length Mean', \n",
    "                'Average Packet Size', \n",
    "                'URG Flag Count', \n",
    "                'Protocol', \n",
    "                'Init_Win_bytes_forward', \n",
    "                'Packet Length Variance', \n",
    "                'Total Fwd Packets', \n",
    "                'Subflow Fwd Packets', \n",
    "                'Bwd Packet Length Std', \n",
    "                'Init_Win_bytes_backward', \n",
    "                'Bwd Packet Length Max', \n",
    "                'Min Packet Length', \n",
    "                'Source Port', \n",
    "                'Destination Port', \n",
    "                'Packet Length Std', \n",
    "                'Bwd Packet Length Mean']\n",
    "cols_to_keep.append('Label')\n",
    "\n",
    "def filter_columns(chk, chknum):\n",
    "    # Clean the data for the features we want  \n",
    "    print('Dropping unnecessary columns for chunk {}...'.format(chknum), end='')\n",
    "    chk.drop(columns=[col for col in chk.columns if col not in cols_to_keep], inplace=True, errors='raise') # Some systems raise an error about this errors='raise' argument (which is ironic, I guess). Feel free to remove it if need be.\n",
    "    chk.dropna(inplace=True)\n",
    "    print('done')\n",
    "    \n",
    "    return chk\n",
    "\n",
    "# Each chunk is in df format\n",
    "chunkn = 1\n",
    "for chunk in df_chunk:  \n",
    "    # perform data filtering \n",
    "    chunk_filter = filter_columns(chunk, chunkn)\n",
    "    chunkn += 1\n",
    "    \n",
    "    # Once the data filtering is done, append the chunk to list\n",
    "    chunk_list.append(chunk_filter)\n",
    "    \n",
    "# concat the list into dataframe \n",
    "df = pd.concat(chunk_list)\n",
    "\n",
    "dep_var = 'Label'\n",
    "print('\\nDataset Composition:\\n{}\\n'.format(df[dep_var].value_counts()))\n",
    "\n",
    "# Output the features we have active\n",
    "print('Active Features:')\n",
    "n=1\n",
    "for col in df.columns[:-1]:\n",
    "    print('\\t{}. {}'.format(n, col))\n",
    "    n+=1\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see the features that have been chosen by the `feature selection` notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "   Source Port  Destination Port  Protocol  Total Fwd Packets  \\\n",
      "0       1167.0              53.0      17.0                1.0   \n",
      "1      39730.0              80.0       6.0                2.0   \n",
      "2      49637.0              80.0       6.0                2.0   \n",
      "3      52537.0             443.0       6.0                1.0   \n",
      "4      48063.0              53.0      17.0                1.0   \n",
      "\n",
      "   Bwd Packet Length Max  Bwd Packet Length Mean  Bwd Packet Length Std  \\\n",
      "0                  139.0                   139.0                0.00000   \n",
      "1                    0.0                     0.0                0.00000   \n",
      "2                    0.0                     0.0                0.00000   \n",
      "3                   31.0                    15.5               21.92031   \n",
      "4                  153.0                   153.0                0.00000   \n",
      "\n",
      "   Fwd PSH Flags  Min Packet Length  Packet Length Mean  ...  \\\n",
      "0            0.0               35.0           69.666667  ...   \n",
      "1            0.0                0.0            0.000000  ...   \n",
      "2            0.0                0.0            0.000000  ...   \n",
      "3            0.0                0.0            7.750000  ...   \n",
      "4            0.0               31.0           71.666667  ...   \n",
      "\n",
      "   Packet Length Variance  SYN Flag Count  ACK Flag Count  URG Flag Count  \\\n",
      "0             3605.333333             0.0             0.0             0.0   \n",
      "1                0.000000             0.0             1.0             0.0   \n",
      "2                0.000000             0.0             1.0             0.0   \n",
      "3              240.250000             0.0             1.0             1.0   \n",
      "4             4961.333333             0.0             0.0             0.0   \n",
      "\n",
      "   Down/Up Ratio  Average Packet Size  Subflow Fwd Packets  \\\n",
      "0            1.0           104.500000                  1.0   \n",
      "1            0.0             0.000000                  2.0   \n",
      "2            0.0             0.000000                  2.0   \n",
      "3            2.0            10.333333                  1.0   \n",
      "4            1.0           107.500000                  1.0   \n",
      "\n",
      "   Init_Win_bytes_forward  Init_Win_bytes_backward   Label  \n",
      "0                    -1.0                     -1.0  BENIGN  \n",
      "1                  3903.0                     -1.0  BENIGN  \n",
      "2                  1386.0                     -1.0  BENIGN  \n",
      "3                  4175.0                   1028.0  BENIGN  \n",
      "4                    -1.0                     -1.0  BENIGN  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Data after encoding:\n",
      "[ 3.07289751e-01  1.39557471e-02  4.47637170e-03  2.63315982e-04\n",
      "  3.66009215e-02  3.66009215e-02  0.00000000e+00  0.00000000e+00\n",
      "  9.21605938e-03  1.83443468e-02  1.58106575e-02  9.49341888e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.63315982e-04\n",
      "  2.75165201e-02  2.63315982e-04 -2.63315982e-04 -2.63315982e-04]  1\n",
      "[ 9.95207272e-01  2.00394115e-03  1.50295586e-04  5.00985287e-05\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.50492643e-05  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  5.00985287e-05  9.77672787e-02 -2.50492643e-05]  1\n",
      "[ 9.99609082e-01  1.61107091e-03  1.20830318e-04  4.02767727e-05\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.01383863e-05  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.02767727e-05  2.79118034e-02 -2.01383863e-05]  1\n",
      "[9.96621800e-01 8.40366708e-03 1.13819419e-04 1.89699031e-05\n",
      " 5.88066997e-04 2.94033498e-04 4.15826161e-04 0.00000000e+00\n",
      " 0.00000000e+00 1.47016749e-04 2.94033498e-04 4.55751922e-03\n",
      " 0.00000000e+00 1.89699031e-05 1.89699031e-05 3.79398062e-05\n",
      " 1.96022332e-04 1.89699031e-05 7.91993455e-02 1.95010604e-02]  1\n",
      "[ 9.94698986e-01  1.09687382e-03  3.51827451e-04  2.06957324e-05\n",
      "  3.16644706e-03  3.16644706e-03  0.00000000e+00  0.00000000e+00\n",
      "  6.41567704e-04  1.48319415e-03  1.45773977e-03  1.02678427e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.06957324e-05\n",
      "  2.22479123e-03  2.06957324e-05 -2.06957324e-05 -2.06957324e-05]  1\n"
     ]
    }
   ],
   "source": [
    "print('Data before encoding:')\n",
    "print(df.head())\n",
    "\n",
    "# Separate the data out into the data and target classification\n",
    "X = normalize(( df.loc[:, df.columns != dep_var] ).values)\n",
    "y = df[dep_var]\n",
    "# One-Hot Encoding for the target classification\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "#y = y.map(lambda classif : 0 if classif == 'BENIGN' else 1)\n",
    "#y = keras.utils.to_categorical(y, num_classes=y.nunique())\n",
    "\n",
    "print('Data after encoding:')\n",
    "for i in range(5):\n",
    "    print('{}  {}'.format(X[i, :], y[i]))\n",
    "#print(X[:5, :])\n",
    "#print('Label column:')\n",
    "#print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing sets\n",
    "Here, we are splitting up the X (data) and y (target) sets into specifically a training and testing set. We fit the models on the training set and then record their performance on the testing set to appropriately determine how the models are generalizing the data coming in. It would also be appropriate here to create an additional validation set or conduct 10-fold cross validation for every model. However, since the `train_test_split` function provided by `sklearn` automatically stratifies the dataset (keeps the target data in proper proportions), this will do for the time being as it is a general statement towards the model's performance. Once the results are needed for a formal paper or presentation, 10-fold cs will be conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset up into training and testing sets\n",
    "#  This split is not stratified by default according to \n",
    "#  the documentation found here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, stratify=y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting and Evaluation\n",
    "The models we are training here are the Random Forest (`RandomForestClassifier`), Decision Tree (`DecisionTreeClassifier`), and k-Nearest Neighbors (`KNeighborsClassifier`) implementations provided by `sklearn`. We can see that the models perform modestly at around 60% accuracy. Just for fun, we are also running the `AdaBoost` model provided by `sklearn`, which extends off of the work of an already-existing model. This `AdaBoost` algorithm doesn't improve the performance of the models enough, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest testing accuracy: 66.69%\n"
     ]
    }
   ],
   "source": [
    "# Random Forest model training and evaluation\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "rf.fit(X_train, y_train)\n",
    "print('Random Forest testing accuracy: {:.2f}%'.format(100*rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree testing accuracy: 64.94%\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree model training and evaluation\n",
    "dt = DecisionTreeClassifier(random_state=random_state)\n",
    "dt.fit(X_train, y_train)\n",
    "print('Decision Tree testing accuracy: {:.2f}%'.format(100*dt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors testing accuracy: 61.09%\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors model training and evaluation\n",
    "knn = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
    "knn.fit(X_train, y_train)\n",
    "print('k-Nearest Neighbors testing accuracy: {:.2f}%'.format(100*knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost-Decision Tree testing accuracy: 65.73%\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost-Decision Tree model training and evaluation\n",
    "adadt = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=random_state)\n",
    "adadt.fit(X_train, y_train)\n",
    "print('AdaBoost-Decision Tree testing accuracy: {:.2f}%'.format(100*adadt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data File\n",
    "Wait...what? Why are we here, again? Well, the way we set up the data for the `sklearn` models is different from the way that `keras` wants their deep learning models trained. Removing the columns and selecting the features is exactly the same as before, however configuring `y` (target) should be categorical, hot-encoded scheme rather than a simple 1D array. It can work either way, however I believe this is the best method for the time being<sup>[citation required]</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unnecessary columns for chunk 1...done\n",
      "Dropping unnecessary columns for chunk 2...done\n",
      "Dropping unnecessary columns for chunk 3...done\n",
      "Dropping unnecessary columns for chunk 4...done\n",
      "Dropping unnecessary columns for chunk 5...done\n",
      "Dropping unnecessary columns for chunk 6...done\n",
      "Dropping unnecessary columns for chunk 7...done\n",
      "Dropping unnecessary columns for chunk 8...done\n",
      "Dropping unnecessary columns for chunk 9...done\n",
      "Dropping unnecessary columns for chunk 10...done\n",
      "ADWARE    229275\n",
      "BENIGN    229275\n",
      "Name: Label, dtype: int64\n",
      "Active Features:\n",
      "\tSource Port\n",
      "\tDestination Port\n",
      "\tProtocol\n",
      "\tTotal Fwd Packets\n",
      "\tBwd Packet Length Max\n",
      "\tBwd Packet Length Mean\n",
      "\tBwd Packet Length Std\n",
      "\tFwd PSH Flags\n",
      "\tMin Packet Length\n",
      "\tPacket Length Mean\n",
      "\tPacket Length Std\n",
      "\tPacket Length Variance\n",
      "\tSYN Flag Count\n",
      "\tACK Flag Count\n",
      "\tURG Flag Count\n",
      "\tDown/Up Ratio\n",
      "\tAverage Packet Size\n",
      "\tSubflow Fwd Packets\n",
      "\tInit_Win_bytes_forward\n",
      "\tInit_Win_bytes_backward\n"
     ]
    }
   ],
   "source": [
    "# Import the data file again because we've had to mess with it and the DL algorithms we're using want the y-data\n",
    "#  in a different format\n",
    "df_chunk = pd.read_csv(path + datafile, chunksize=50000)\n",
    "chunk_list = []  # append each chunk df here \n",
    "\n",
    "# Each chunk is in df format\n",
    "chunkn = 1\n",
    "for chunk in df_chunk:  \n",
    "    # perform data filtering \n",
    "    chunk_filter = filter_columns(chunk, chunkn)\n",
    "    chunkn += 1\n",
    "    \n",
    "    # Once the data filtering is done, append the chunk to list\n",
    "    chunk_list.append(chunk_filter)\n",
    "    \n",
    "# concat the list into dataframe \n",
    "df = pd.concat(chunk_list)\n",
    "\n",
    "dep_var = 'Label'\n",
    "print(df[dep_var].value_counts())\n",
    "#print('Available features: {}'.format(df.columns))\n",
    "\n",
    "# Output the features we have active\n",
    "print('Active Features:')\n",
    "for col in df.columns[:-1]:\n",
    "    print('\\t' + col)\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "[[ 3.07289751e-01  1.39557471e-02  4.47637170e-03  2.63315982e-04\n",
      "   3.66009215e-02  3.66009215e-02  0.00000000e+00  0.00000000e+00\n",
      "   9.21605938e-03  1.83443468e-02  1.58106575e-02  9.49341888e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  2.63315982e-04\n",
      "   2.75165201e-02  2.63315982e-04 -2.63315982e-04 -2.63315982e-04]\n",
      " [ 9.95207272e-01  2.00394115e-03  1.50295586e-04  5.00985287e-05\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  2.50492643e-05  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  5.00985287e-05  9.77672787e-02 -2.50492643e-05]\n",
      " [ 9.99609082e-01  1.61107091e-03  1.20830318e-04  4.02767727e-05\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  2.01383863e-05  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  4.02767727e-05  2.79118034e-02 -2.01383863e-05]\n",
      " [ 9.96621800e-01  8.40366708e-03  1.13819419e-04  1.89699031e-05\n",
      "   5.88066997e-04  2.94033498e-04  4.15826161e-04  0.00000000e+00\n",
      "   0.00000000e+00  1.47016749e-04  2.94033498e-04  4.55751922e-03\n",
      "   0.00000000e+00  1.89699031e-05  1.89699031e-05  3.79398062e-05\n",
      "   1.96022332e-04  1.89699031e-05  7.91993455e-02  1.95010604e-02]\n",
      " [ 9.94698986e-01  1.09687382e-03  3.51827451e-04  2.06957324e-05\n",
      "   3.16644706e-03  3.16644706e-03  0.00000000e+00  0.00000000e+00\n",
      "   6.41567704e-04  1.48319415e-03  1.45773977e-03  1.02678427e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  2.06957324e-05\n",
      "   2.22479123e-03  2.06957324e-05 -2.06957324e-05 -2.06957324e-05]]\n",
      "Targets:\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Separate the data out into the data and target classification\n",
    "X = normalize( ( df.loc[:, df.columns != dep_var] ).values )\n",
    "y = df[dep_var]\n",
    "# One-Hot Encoding for the target classification\n",
    "label_encoder = LabelEncoder()\n",
    "#y = label_encoder.fit_transform(y)\n",
    "y = y.map(lambda classif : 0 if classif == 'BENIGN' else 1)\n",
    "y = keras.utils.to_categorical(y, num_classes=y.nunique())\n",
    "\n",
    "print('Data:')\n",
    "print(X[:5, :])\n",
    "print('Targets:')\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing data\n",
    "This is the same idea as before.  \n",
    "<pre>\n",
    "\n",
    "    <i>\"How can you really say that the data \n",
    "    sets are the same as before? Is that \n",
    "    just bad science? Who's running this \n",
    "    circus?\"</i>\n",
    "    - you\n",
    "\n",
    "</pre>\n",
    "I hear the concerns that you is expressing. However, thanks to someone smarter<sub>(everyday?)</sub> than me, by passing the same `random_state` value, I can control this variable such that I get back the same splits as long as I'm providing the same data (which I am)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset up into training and testing sets\n",
    "#  This split is not stratified by default according to \n",
    "#  the documentation found here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, stratify=y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network Model Training and Evaluation\n",
    "Now, we begin with the crazier stuff. We've set up a `Sequential` model from the `keras` library and added a bunch of `Dense` layers to it. Playing around with the activation and loss functions, this is what we've currently settled on. As we can see, though, the model still only performs as well as flipping a coin (performing worse than some of the simpler models we saw before). We are still investigating potentially solutions to this problem and haven't had much luck in the way of success. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: accuracy: 55.12%: loss: 67.918986\n",
      "\tTrue Positive Rate: 0.40410845746157203 (122301.0)\n",
      "\tTrue Negative Rate: 0.12559310609164562 (38010.0)\n",
      "\tFalse Positive Rate: 0.37440689390835435 (113312.0)\n",
      "\tFalse Negative Rate: 0.09589154253842799 (29021.0)\n"
     ]
    }
   ],
   "source": [
    "# Deep Neural Network model training and evaluation\n",
    "\n",
    "# Set up the metrics we want to collect\n",
    "accuracy = CategoricalAccuracy() \t# Will change this to Categorical if the target classification is categorical\n",
    "tp = TruePositives()\t\t# These could be collected with a confusion matrix, however translating back\n",
    "tn = TrueNegatives()\t\t#  and forth from an image may be frustrating (it was last time I did it)\n",
    "fp = FalsePositives()\n",
    "fn = FalseNegatives()\n",
    "metrics = [accuracy, tp, tn, fp, fn]\n",
    "\n",
    "# The model must be reinitialized otherwise the model will have trained on all of the data (that wouldn't be true 10-fold cv)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(len(cols_to_keep) - 1 ,))) \t# Input layer, needs same shape as input data (9 values 1D)\n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(Dense(128))\t\t\t# Hidden layer of nodes\n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(Dense(32))\t\t\t# Hidden layer of nodes\n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(Dense(8))\t\t\t# Hidden layer of nodes\n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(Dense(2, activation='sigmoid'))\t\t\t# Output layer of 3 nodes \n",
    "\n",
    "# \"Configures the model for training\"\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
    "    \n",
    "# Fit and test the model \n",
    "model.fit(x=X_train, y=y_train, epochs=20, batch_size=512, verbose=0, validation_data=(X_test, y_test))\n",
    "    \n",
    "# Evaluate the performance of the model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "acc, loss, tpn, tnn, fpn, fnn = scores[1]*100, scores[0]*100, scores[2], scores[3], scores[4], scores[5]\n",
    "totaln = tpn + tnn + fpn + fnn\n",
    "print('Baseline: accuracy: {:.2f}%: loss: {:2f}'.format(acc, loss))\n",
    "print('\\tTrue Positive Rate: {} ({})'.format(tpn/totaln, tpn))\n",
    "print('\\tTrue Negative Rate: {} ({})'.format(tnn/totaln, tnn))\n",
    "print('\\tFalse Positive Rate: {} ({})'.format(fpn/totaln, fpn))\n",
    "print('\\tFalse Negative Rate: {} ({})'.format(fnn/totaln, fnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Really, the moral of the story right now is <b>growth</b>. <i>Is this the best data science possible?</i> No. But, from here, we can find ways to improve our models and <b>grow</b> as humans. While, right now, we don't have the highest-performing models and we aren't using the most l33t techniques, <i>we can always improve ourselves</i>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
